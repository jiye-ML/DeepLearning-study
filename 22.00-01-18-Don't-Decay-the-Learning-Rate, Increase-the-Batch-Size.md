### Don't Decay the Learning Rate, Increase the Batch Size
* [paper](paper/22.00-01-18-Don't-Decay-the-Learning-Rate-Increase-the-Batch-Size.pdf)
* [学界 | 抛弃Learning Rate Decay吧！](https://mp.weixin.qq.com/s/wUihQ7uYH4rUQ4gnFnLdDw)
    * 摘要：
        1. 实际上作者在衰减学习率的时候同时也降低了SGD中随机波动的值；衰减学习率类似于模拟退火；
        2. 不同于衰减学习率，提出了在增加 Batch Size 的同时保持学习率的策略，既可以保证不掉点，还可以减少参数更新的次数；
        3. 增加学习率又增大 Batch Size，如此可以基本保持test中不掉点又进一步减少参数更新次数；
        4. 对比了自己的模型和另一篇著名论文(Accurate, large minibatch SGD: Training imagenet in 1 hour)中的模型，
        Batch Size：65536 - 8192；正确率：77% - 76%；参数更新次数：2500 - 14000；