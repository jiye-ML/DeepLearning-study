 https://github.com/jiye-ML/tensorflow-classification-network/blob/master/21.02-00-15-inception-v2.md 




[v2] Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift

- paper
- BN本质上解决的是反向传播过程中的梯度问题。
  
- 《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》论文解读
  1. 归一化：
     - 原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；
     - 一旦每批训练数据的分布各不相同，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度；
  2. 传递性
     - 网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。
     - 一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，
训练数据的分布一直在发生变化，那么将会影响网络的训练速度。
- 基础 | batchnorm原理及代码详解
  - 每层输出的均值和归一化处理





- 《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》阅读笔记与实现
- 
