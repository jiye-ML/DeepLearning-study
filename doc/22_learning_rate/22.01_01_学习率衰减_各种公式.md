学习率衰减：learning rate decay

直观理解：如果在整个梯度下降过程中，保持learning rate不变，如果learning rate设置小了，会导致梯度下降过慢，如果设置大了，对于mini-batch来说最后就很难收敛，一直在最小值附近盘旋。所以动态改变learning rate很重要，在开始的时候，设置较大的learning rate，可以保证梯度下降的速度，慢慢减小，可以使最后的cost function在最小值非常小的范围内盘旋，得到一个比较满意的值。



对learning rate处理的公式有很多，距离如下：

![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7B0%7D) 是初始learning rate

![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3D%5Cfrac%7B1%7D%7B1%2Bdecayrate%5Cast+epoch%7D%5Calpha_%7B0%7D) decayrate超参、epoch是mini-batch的迭代次数

![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3D0.95%5E%7Bepoch%7D+%5Cast+%5Calpha_%7B0%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3D%5Cfrac%7Bk%7D%7B%5Csqrt%7Bepoch%7D%7D+%5Cast+%5Calpha_%7B0%7D) k是常量

![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3D%5Cfrac%7Bk%7D%7B%5Csqrt%7Bt%7D%7D+%5Cast+%5Calpha_%7B0%7D) t是mini-batch num ？？？

离散 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 会过一段时间就减半



关于 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 的调节问题：

在模型小的时候，有的人会不断观察模型在训练过程中的变化情况开手动修改 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) ，这个出可行的，有些人这么做。