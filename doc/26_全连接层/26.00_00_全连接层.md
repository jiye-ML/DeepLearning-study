## 写在前面

* [全连接层的作用是什么？](https://www.zhihu.com/question/41037974)
    * 全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。
    如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，
    全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。
    在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；
    而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽;
    * 目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet
    等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，
    最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能。
    * FC可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在ImageNet上预训练得到的模型为 ，
    则ImageNet可视为源域（迁移学习中的source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。
    针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比ImageNet，目标域图像不是物体为中心的图像，
    而是风景照，见下图），不含FC的网络微调后的结果要差于含FC的网络。因此FC可视作模型表示能力的“防火墙”，
    特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。）

# 卷积层代替全连接层
* https://zhuanlan.zhihu.com/p/82739829